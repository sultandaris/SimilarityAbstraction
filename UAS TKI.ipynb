{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e31064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelompok UAS TKI\n",
      "file Docs1.txt:\n",
      "Penyakit jantung merupakan\n",
      "\n",
      "file Docs2.txt:\n",
      "Perkembangan industri video\n",
      "\n",
      "file Docs3.txt:\n",
      "Sulitnya penyedia layanan\n",
      "\n",
      "file Docs4.txt:\n",
      "Sulitnya penyedia layanan\n",
      "\n",
      "Preprocessing file: Docs1.txt\n",
      "content of Docs1.txt after preprocessing and stemming:\n",
      "sakit jantung rupa sebab mati nomor dunia\n",
      "telah banyak laku teliti kait manfaat machine\n",
      "learning prediksi sakit jantung machine learning bagi\n",
      "teliti tentang cerdas buat cari beri tahu\n",
      "kepada komputer lalu data amat interaksi dunia\n",
      "dapat bagai macam metode manfaat machine learning\n",
      "antara na ve bayes decision tree guna teliti\n",
      "teliti tuju tingkat akurasi studi kasus prediksi\n",
      "sakit jantung teliti belum tambah jumlah data\n",
      "bukti metode mana beri akurasi lebih baik\n",
      "antara na ve bayes decision tree data guna\n",
      "rupa data sekunder dapat website kaggle isi\n",
      "subset atribut diri atribut bagai predictor atribut\n",
      "bagai target kelas jumlah 1328 data data\n",
      "kemudian klasifikasi cari nilai akurasi sensitivitas spesifisitas\n",
      "tinggi dua metode evaluasi sistem laku guna\n",
      "confusion matrix fold cross validation kurva roc\n",
      "hitung nilai area under curve auc hasil\n",
      "evaluasi sistem oleh nilai tinggi metode decision\n",
      "tree uji kurva roc confusion matrix nilai\n",
      "fold rata rata nilai akurasi besar sensitivitas\n",
      "besar spesifisitas besar auc besar 979 sedang\n",
      "metode na ve bayes oleh hasil paling tinggi\n",
      "uji kurva roc confusion matrix nilai fold\n",
      "rata rata nilai akurasi besar sensitivitas besar\n",
      "spesifisitas besar auc besar 832 kata kunci\n",
      "na ve bayes decision tree heart disease classification\n",
      "confusion matrix kurva roc\n",
      "Original words: 265\n",
      "After preprocessing: 193\n",
      "After stemming: 193\n",
      "Unique words: 97\n",
      "Saved to: preprocessed_Docs1.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs2.txt\n",
      "content of Docs2.txt after preprocessing and stemming:\n",
      "kembang industri video games dewasa makin pesat\n",
      "mudah akses sehingga makin banyak masyarakat kenal\n",
      "video games utama jenis battle royale sedang\n",
      "populer dewasa banyak debat kena dampak jenis\n",
      "video game sebut lebih kasus keras tembak\n",
      "jadi belakang banyak hubung dampak timbul video\n",
      "games teliti tuju menganalisa sentimen guna twitter\n",
      "kena dampak video game jenis battle royale\n",
      "guna metode support vector machine svm data\n",
      "ambil tweet guna twitter kena dampak video\n",
      "games jumlah 513 tweet bahasa inggris kemudian\n",
      "sentimen kategori jadi sentimen positif negatif dasar\n",
      "hasil uji akurasi besar dapat guna svm\n",
      "kernel polynomial besar skenario banding data training\n",
      "data testing hasil sentimen dapat yaitu sentimen\n",
      "positif sedang sentimen negatif kata kunci analisis\n",
      "sentimen twitter video games battle royale support\n",
      "vector machine svm\n",
      "Original words: 152\n",
      "After preprocessing: 122\n",
      "After stemming: 122\n",
      "Unique words: 70\n",
      "Saved to: preprocessed_Docs2.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs3.txt\n",
      "content of Docs3.txt after preprocessing and stemming:\n",
      "sulit sedia layan game tahan main waktu\n",
      "panjang jadi masalah sendiri main game nilai\n",
      "terlalu cepat pindah pindah main game oleh\n",
      "karena tuju teliti uji bagaimana karakteristik relationship\n",
      "length durasi depth tingkat guna breadth beli\n",
      "pengaruh loyalitas main peran relationship switching cost\n",
      "biaya alih gaming habit biasa main dampak\n",
      "sebut social capital self perception theory guna\n",
      "jelas teliti awal laku sebar kuesioner telah\n",
      "evaluasi baik kuesioner sebar cara luas komunitas\n",
      "komunitas main cara online dapat 453 responden\n",
      "kemudian data responden kumpul analis guna covariance\n",
      "based structural equation model sem tools lisrel\n",
      "hasil analisis tunjuk variabel relation switching cost\n",
      "gaming habit pengaruh signifikan hadap loyalitas main\n",
      "tetapi variabel relationship length breadth relational switching\n",
      "cost pengaruh gaming habit dasar hasil analisis\n",
      "game developer game publisher tingkat fitur fitur\n",
      "game pengaruh loyalitas main kata kunci online\n",
      "game mobile social capital self perception loyalitas\n",
      "main\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After stemming: 141\n",
      "Unique words: 90\n",
      "Saved to: preprocessed_Docs3.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs4.txt\n",
      "content of Docs4.txt after preprocessing and stemming:\n",
      "sulit sedia layan game tahan main waktu\n",
      "panjang jadi masalah sendiri main game nilai\n",
      "terlalu cepat pindah pindah main game oleh\n",
      "karena tuju teliti uji bagaimana karakteristik relationship\n",
      "length durasi depth tingkat guna breadth beli\n",
      "pengaruh loyalitas main peran relationship switching cost\n",
      "biaya alih gaming habit biasa main dampak\n",
      "sebut social capital self perception theory guna\n",
      "jelas teliti awal laku sebar kuesioner telah\n",
      "evaluasi baik kuesioner sebar cara luas komunitas\n",
      "komunitas main cara online dapat 453 responden\n",
      "kemudian data responden kumpul analis guna covariance\n",
      "based structural equation model sem tools lisrel\n",
      "hasil analisis tunjuk variabel relation switching cost\n",
      "gaming habit pengaruh signifikan hadap loyalitas main\n",
      "tetapi variabel relationship length breadth relational switching\n",
      "cost pengaruh gaming habit dasar hasil analisis\n",
      "game developer game publisher tingkat fitur fitur\n",
      "game pengaruh loyalitas main kata kunci online\n",
      "game mobile social capital self perception loyalitas\n",
      "main\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After stemming: 141\n",
      "Unique words: 90\n",
      "Saved to: preprocessed_Docs4.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "print(\"Kelompok UAS TKI\")\n",
    "\n",
    "folder_path = r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\Dokumen\" \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            words = content.split()\n",
    "            first_10_words = ' '.join(words[:3])\n",
    "            print(f\"file {filename}:\\n{first_10_words}\\n\")\n",
    "            processed_documents_sastrawi = {}\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    print(f\"Preprocessing file: {filename}\")\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "                    \n",
    "            content = content.lower()\n",
    "            \n",
    "            # Segmentasi teks - pisahkan menjadi kalimat berdasarkan tanda titik\n",
    "            sentences = re.split(r'[.!?]+', content)\n",
    "            \n",
    "            content = re.sub(r'[^\\w\\s]', ' ', content)\n",
    "            \n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            words = content.split()\n",
    "            \n",
    "            stop_words = {'dan', 'atau', 'yang', 'adalah', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'dalam', 'dari', 'ke', 'di', 'akan', 'dapat', 'juga', 'tidak', 'ada', 'satu', 'dua', 'tiga'}\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "            \n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            \n",
    "            processed_documents_sastrawi[filename] = stemmed_words\n",
    "            \n",
    "            segmented_lines = []\n",
    "            for i in range(0, len(stemmed_words), 7):\n",
    "                line = ' '.join(stemmed_words[i:i+7])\n",
    "                segmented_lines.append(line)\n",
    "            \n",
    "            segmented_output = '\\n'.join(segmented_lines)\n",
    "            \n",
    "            output_filename = f\"preprocessed_{filename}\"\n",
    "            output_path = os.path.join(r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\DokumenSastrawi\", output_filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(segmented_output)\n",
    "            \n",
    "            print(f\"content of {filename} after preprocessing and stemming:\")\n",
    "            print(segmented_output)\n",
    "            print(f\"Original words: {len(words)}\")\n",
    "            print(f\"After preprocessing: {len(filtered_words)}\")\n",
    "            print(f\"After stemming: {len(stemmed_words)}\")\n",
    "            unique_words = len(set(stemmed_words))\n",
    "            print(f\"Unique words: {unique_words}\")\n",
    "            print(f\"Saved to: {output_filename}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef08b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelompok UAS TKI\n",
      "Preprocessing file: Docs1.txt\n",
      "content of Docs1.txt after preprocessing and Porter stemming:\n",
      "penyakit jantung merupakan penyebab kematian nomor dunia\n",
      "telah banyak dilakukan penelitian terkait pemanfaatan machin\n",
      "learn memprediksi penyakit jantung machin learn bagian\n",
      "penelitian tentang kecerdasan buatan mencari memberikan pengetahuan\n",
      "kepada komput melalui data pengamatan berinteraksi dunia\n",
      "terdapat berbagai macam metod pemanfaatan machin learn\n",
      "diantaranya na誰v bay decis tree digunakan penelitian\n",
      "penelitian bertujuan meningkatkan akurasi studi kasu prediksi\n",
      "penyakit jantung penelitian sebelumnya menambahkan jumlah data\n",
      "membuktikan metod mana memberikan akurasi lebih baik\n",
      "antara na誰v bay decis tree data digunakan\n",
      "merupakan data sekund didapat websit kaggl berisi\n",
      "subset atribut terdiri atribut sebagai predictor atribut\n",
      "sebagai target kela jumlah 1328 data data\n",
      "kemudian diklasifikasian mencari nilai akurasi sensitivita spesifisita\n",
      "tertinggi kedua metod evaluasi sistem dilakukan menggunakan\n",
      "confus matrix fold cross valid kurva roc\n",
      "menghitung nilai area under curv auc hasil\n",
      "evaluasi sistem diperoleh nilai tertinggi metod decis\n",
      "tree pengujian kurva roc confus matrix nilai\n",
      "fold rata rata nilai akurasi sebesar sensitivita\n",
      "sebesar spesifisita sebesar auc sebesar 979 sedangkan\n",
      "metod na誰v bay memperoleh hasil pale tinggi\n",
      "pengujian kurva roc confus matrix nilai fold\n",
      "rata rata nilai akurasi sebesar sensitivita sebesar\n",
      "spesifisita sebesar auc sebesar 832 kata kunci\n",
      "na誰v bay decis tree heart diseas classif\n",
      "confus matrix kurva roc\n",
      "Original words: 265\n",
      "After preprocessing: 193\n",
      "After Porter stemming: 193\n",
      "Unique words: 104\n",
      "Saved to: preprocessed_Docs1.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs2.txt\n",
      "content of Docs2.txt after preprocessing and Porter stemming:\n",
      "perkembangan industri video game dewasa semakin pesat\n",
      "mudah diaks sehingga semakin banyak masyarakat mengen\n",
      "video game terutama berjeni battl royal sedang\n",
      "popul dewasa banyak perdebatan mengenai dampak jeni\n",
      "video game tersebut terlebih kasu kekerasan penembakan\n",
      "terjadi belakangan banyak dihubungkan dampak ditimbulkan video\n",
      "game penelitian bertujuan menganalisa sentimen pengguna twitter\n",
      "mengenai dampak video game berjeni battl royal\n",
      "menggunakan metod support vector machin svm data\n",
      "diambil tweet pengguna twitter mengenai dampak video\n",
      "game berjumlah 513 tweet berbahasa inggri kemudian\n",
      "sentimen dikategorikan menjadi sentimen positif negatif berdasarkan\n",
      "hasil pengujian akurasi terbesar didapat menggunakan svm\n",
      "kernel polynomi sebesar skenario perbandingan data train\n",
      "data test hasil sentimen didapatkan yaitu bersentimen\n",
      "positif sedangkan bersentimen negatif kata kunci analisi\n",
      "sentimen twitter video game battl royal support\n",
      "vector machin svm\n",
      "Original words: 152\n",
      "After preprocessing: 122\n",
      "After Porter stemming: 122\n",
      "Unique words: 76\n",
      "Saved to: preprocessed_Docs2.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs3.txt\n",
      "content of Docs3.txt after preprocessing and Porter stemming:\n",
      "sulitnya penyedia layanan game mempertahankan pemain waktu\n",
      "panjang menjadi masalah tersendiri pemain game dinilai\n",
      "terlalu cepat berpindah pindah bermain game oleh\n",
      "karena tujuan penelitian menguji bagaimana karakteristik relationship\n",
      "length durasi depth peningkatan penggunaan breadth pembelian\n",
      "mempengaruhi loyalita pemain peran relationship switch cost\n",
      "biaya peralihan game habit kebiasaan bermain dampak\n",
      "tersebut social capit self percept theori digunakan\n",
      "menjelaskan penelitian diawali melakukan penyebaran kuesion telah\n",
      "dievaluasi baik kuesion disebarkan secara lua komunita\n",
      "komunita pemain secara onlin didapatkan 453 responden\n",
      "kemudian data responden terkumpul dianalisi menggunakan covari\n",
      "base structur equat model sem tool lisrel\n",
      "hasil analisi menunjukkan variabel relat switch cost\n",
      "game habit berpengaruh signifikan terhadap loyalita pemain\n",
      "tetapi variabel relationship length breadth relat switch\n",
      "cost mempengaruhi game habit berdasarkan hasil analisi\n",
      "game develop game publish meningkatkan fitur fitur\n",
      "game mempengaruhi loyalita pemain kata kunci onlin\n",
      "game mobil social capit self percept loyalita\n",
      "pemain\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After Porter stemming: 141\n",
      "Unique words: 95\n",
      "Saved to: preprocessed_Docs3.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs4.txt\n",
      "content of Docs4.txt after preprocessing and Porter stemming:\n",
      "sulitnya penyedia layanan game mempertahankan pemain waktu\n",
      "panjang menjadi masalah tersendiri pemain game dinilai\n",
      "terlalu cepat berpindah pindah bermain game oleh\n",
      "karena tujuan penelitian menguji bagaimana karakteristik relationship\n",
      "length durasi depth peningkatan penggunaan breadth pembelian\n",
      "mempengaruhi loyalita pemain peran relationship switch cost\n",
      "biaya peralihan game habit kebiasaan bermain dampak\n",
      "tersebut social capit self percept theori digunakan\n",
      "menjelaskan penelitian diawali melakukan penyebaran kuesion telah\n",
      "dievaluasi baik kuesion disebarkan secara lua komunita\n",
      "komunita pemain secara onlin didapatkan 453 responden\n",
      "kemudian data responden terkumpul dianalisi menggunakan covari\n",
      "base structur equat model sem tool lisrel\n",
      "hasil analisi menunjukkan variabel relat switch cost\n",
      "game habit berpengaruh signifikan terhadap loyalita pemain\n",
      "tetapi variabel relationship length breadth relat switch\n",
      "cost mempengaruhi game habit berdasarkan hasil analisi\n",
      "game develop game publish meningkatkan fitur fitur\n",
      "game mempengaruhi loyalita pemain kata kunci onlin\n",
      "game mobil social capit self percept loyalita\n",
      "pemain\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After Porter stemming: 141\n",
      "Unique words: 95\n",
      "Saved to: preprocessed_Docs4.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data if not already downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Kelompok UAS TKI\")\n",
    "\n",
    "# Initialize Porter stemmer instead of Sastrawi\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Reset processed_documents\n",
    "processed_documents = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    print(f\"Preprocessing file: {filename}\")\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "                    \n",
    "            content = content.lower()\n",
    "            \n",
    "            # Segmentasi teks - pisahkan menjadi kalimat berdasarkan tanda titik\n",
    "            sentences = re.split(r'[.!?]+', content)\n",
    "            \n",
    "            content = re.sub(r'[^\\w\\s]', ' ', content)\n",
    "            \n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            words = content.split()\n",
    "            \n",
    "            stop_words = {'dan', 'atau', 'yang', 'adalah', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'dalam', 'dari', 'ke', 'di', 'akan', 'dapat', 'juga', 'tidak', 'ada', 'satu', 'dua', 'tiga'}\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "            \n",
    "            # Use Porter stemmer instead of Sastrawi\n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            \n",
    "            processed_documents[filename] = stemmed_words\n",
    "            \n",
    "            segmented_lines = []\n",
    "            for i in range(0, len(stemmed_words), 7):\n",
    "                line = ' '.join(stemmed_words[i:i+7])\n",
    "                segmented_lines.append(line)\n",
    "            \n",
    "            segmented_output = '\\n'.join(segmented_lines)\n",
    "            \n",
    "            output_filename = f\"preprocessed_{filename}\"\n",
    "            output_path = os.path.join(r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\DokumenPorter\", output_filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(segmented_output)\n",
    "            \n",
    "            print(f\"content of {filename} after preprocessing and Porter stemming:\")\n",
    "            print(segmented_output)\n",
    "            print(f\"Original words: {len(words)}\")\n",
    "            print(f\"After preprocessing: {len(filtered_words)}\")\n",
    "            print(f\"After Porter stemming: {len(stemmed_words)}\")\n",
    "            unique_words = len(set(stemmed_words))\n",
    "            print(f\"Unique words: {unique_words}\")\n",
    "            print(f\"Saved to: {output_filename}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for Docs1.txt:\n",
      "besar: 8\n",
      "nilai: 7\n",
      "data: 6\n",
      "teliti: 5\n",
      "metode: 5\n",
      "akurasi: 5\n",
      "na ve: 4\n",
      "bayes: 4\n",
      "decision: 4\n",
      "tree: 4\n",
      "\n",
      "\n",
      "Word count for Docs2.txt:\n",
      "video: 7\n",
      "sentimen: 7\n",
      "games: 5\n",
      "dampak: 4\n",
      "guna: 4\n",
      "banyak: 3\n",
      "jenis: 3\n",
      "battle: 3\n",
      "royale: 3\n",
      "kena: 3\n",
      "\n",
      "\n",
      "Word count for Docs3.txt:\n",
      "main: 9\n",
      "game: 7\n",
      "pengaruh: 4\n",
      "loyalitas: 4\n",
      "relationship: 3\n",
      "guna: 3\n",
      "switching: 3\n",
      "cost: 3\n",
      "gaming: 3\n",
      "habit: 3\n",
      "\n",
      "\n",
      "Word count for Docs4.txt:\n",
      "main: 9\n",
      "game: 7\n",
      "pengaruh: 4\n",
      "loyalitas: 4\n",
      "relationship: 3\n",
      "guna: 3\n",
      "switching: 3\n",
      "cost: 3\n",
      "gaming: 3\n",
      "habit: 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename, words in processed_documents.items():\n",
    "    word_count = Counter(words)\n",
    "    print(f\"Word count for {filename}:\")\n",
    "    for word, count in word_count.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c991dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Mean similarity score for Docs1.txt: 0.63\n",
      "Mean similarity score for Docs2.txt: 0.57\n",
      "Mean similarity score for Docs3.txt: 0.73\n",
      "Mean similarity score for Docs4.txt: 0.73\n"
     ]
    }
   ],
   "source": [
    "def rabin_karp_ngrams(words, n, base=256, mod=101):\n",
    "    hashes = set()\n",
    "    if len(words) < n:\n",
    "        return hashes\n",
    "    h = 0\n",
    "    high_order = pow(base, n-1, mod)\n",
    "    \n",
    "    for i in range(n):\n",
    "        h = (h * base + ord(words[i][0])) % mod\n",
    "    hashes.add(h)\n",
    "\n",
    "    for i in range(1, len(words) - n + 1):\n",
    "        h = (h - ord(words[i-1][0]) * high_order) % mod\n",
    "        h = (h * base + ord(words[i+n-1][0])) % mod\n",
    "        hashes.add(h)\n",
    "    return hashes\n",
    "\n",
    "doc_scores = {doc: [] for doc in processed_documents}\n",
    "doc_names = list(processed_documents.keys())\n",
    "ngram_hashes_sastrawi = {doc: rabin_karp_ngrams(processed_documents[doc], n=3) for doc in doc_names}\n",
    "ngram_hashes = {doc: rabin_karp_ngrams(processed_documents[doc], n=3) for doc in doc_names}\n",
    "\n",
    "for i in range(len(doc_names)):\n",
    "    for j in range(i+1, len(doc_names)):\n",
    "        doc1, doc2 = doc_names[i], doc_names[j]\n",
    "        common = ngram_hashes[doc1] & ngram_hashes[doc2]\n",
    "        total = ngram_hashes[doc1] | ngram_hashes[doc2]\n",
    "        similarity = len(common) / len(total) if total else 0\n",
    "        doc_scores[doc1].append(similarity)\n",
    "        doc_scores[doc2].append(similarity)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for doc, scores in doc_scores.items():\n",
    "    mean_score = sum(scores) / len(scores) if scores else 0\n",
    "    print(f\"Mean similarity score for {doc}: {mean_score:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
