{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e31064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASTRAWI STEMMER\n",
      "file Docs1.txt:\n",
      "Penyakit jantung merupakan\n",
      "\n",
      "file Docs2.txt:\n",
      "Perkembangan industri video\n",
      "\n",
      "file Docs3.txt:\n",
      "Sulitnya penyedia layanan\n",
      "\n",
      "file Docs4.txt:\n",
      "Sulitnya penyedia layanan\n",
      "\n",
      "Preprocessing file: Docs1.txt\n",
      "Original words: 265\n",
      "After preprocessing: 193\n",
      "After stemming: 193\n",
      "Unique words: 97\n",
      "Saved to: preprocessed_Docs1.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs2.txt\n",
      "Original words: 152\n",
      "After preprocessing: 122\n",
      "After stemming: 122\n",
      "Unique words: 70\n",
      "Saved to: preprocessed_Docs2.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs3.txt\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After stemming: 141\n",
      "Unique words: 90\n",
      "Saved to: preprocessed_Docs3.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs4.txt\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After stemming: 141\n",
      "Unique words: 90\n",
      "Saved to: preprocessed_Docs4.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "print(\"SASTRAWI STEMMER\")\n",
    "\n",
    "folder_path = r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\Dokumen\" \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            words = content.split()\n",
    "            first_10_words = ' '.join(words[:3])\n",
    "            print(f\"file {filename}:\\n{first_10_words}\\n\")\n",
    "\n",
    "processed_documents_sastrawi = {}\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    print(f\"Preprocessing file: {filename}\")\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "                    \n",
    "            content = content.lower()\n",
    "            \n",
    "            sentences = re.split(r'[.!?]+', content)\n",
    "            \n",
    "            content = re.sub(r'[^\\w\\s]', ' ', content)\n",
    "            \n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            words = content.split()\n",
    "            \n",
    "            stop_words = {'dan', 'atau', 'yang', 'adalah', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'dalam', 'dari', 'ke', 'di', 'akan', 'dapat', 'juga', 'tidak', 'ada', 'satu', 'dua', 'tiga'}\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "            \n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            \n",
    "            processed_documents_sastrawi[filename] = stemmed_words\n",
    "            \n",
    "            segmented_lines = []\n",
    "            for i in range(0, len(stemmed_words), 7):\n",
    "                line = ' '.join(stemmed_words[i:i+7])\n",
    "                segmented_lines.append(line)\n",
    "            \n",
    "            segmented_output = '\\n'.join(segmented_lines)\n",
    "            \n",
    "            output_filename = f\"preprocessed_{filename}\"\n",
    "            output_path = os.path.join(r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\DokumenSastrawi\", output_filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(segmented_output)\n",
    "            \n",
    "           \n",
    "            print(f\"Original words: {len(words)}\")\n",
    "            print(f\"After preprocessing: {len(filtered_words)}\")\n",
    "            print(f\"After stemming: {len(stemmed_words)}\")\n",
    "            unique_words = len(set(stemmed_words))\n",
    "            print(f\"Unique words: {unique_words}\")\n",
    "            print(f\"Saved to: {output_filename}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef08b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORTER STEMMER\n",
      "Preprocessing file: Docs1.txt\n",
      "Original words: 265\n",
      "After preprocessing: 193\n",
      "After Porter stemming: 193\n",
      "Unique words: 104\n",
      "Saved to: preprocessed_Docs1.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs2.txt\n",
      "Original words: 152\n",
      "After preprocessing: 122\n",
      "After Porter stemming: 122\n",
      "Unique words: 76\n",
      "Saved to: preprocessed_Docs2.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs3.txt\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After Porter stemming: 141\n",
      "Unique words: 95\n",
      "Saved to: preprocessed_Docs3.txt\n",
      "\n",
      "\n",
      "Preprocessing file: Docs4.txt\n",
      "Original words: 174\n",
      "After preprocessing: 141\n",
      "After Porter stemming: 141\n",
      "Unique words: 95\n",
      "Saved to: preprocessed_Docs4.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"PORTER STEMMER\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "processed_documents_porter = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    print(f\"Preprocessing file: {filename}\")\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "                    \n",
    "            content = content.lower()\n",
    "            \n",
    "            sentences = re.split(r'[.!?]+', content)\n",
    "            \n",
    "            content = re.sub(r'[^\\w\\s]', ' ', content)\n",
    "            \n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            words = content.split()\n",
    "            \n",
    "            stop_words = {'dan', 'atau', 'yang', 'adalah', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'dalam', 'dari', 'ke', 'di', 'akan', 'dapat', 'juga', 'tidak', 'ada', 'satu', 'dua', 'tiga'}\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "            \n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            \n",
    "            processed_documents_porter[filename] = stemmed_words\n",
    "            \n",
    "            segmented_lines = []\n",
    "            for i in range(0, len(stemmed_words), 7):\n",
    "                line = ' '.join(stemmed_words[i:i+7])\n",
    "                segmented_lines.append(line)\n",
    "            \n",
    "            segmented_output = '\\n'.join(segmented_lines)\n",
    "            \n",
    "            output_filename = f\"preprocessed_{filename}\"\n",
    "            output_path = os.path.join(r\"C:\\Users\\Sultan Daris\\Downloads\\UAS TKI Projek\\DokumenAbstrak\\DokumenPorter\", output_filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(segmented_output)\n",
    "            \n",
    "            print(f\"Original words: {len(words)}\")\n",
    "            print(f\"After preprocessing: {len(filtered_words)}\")\n",
    "            print(f\"After Porter stemming: {len(stemmed_words)}\")\n",
    "            unique_words = len(set(stemmed_words))\n",
    "            print(f\"Unique words: {unique_words}\")\n",
    "            print(f\"Saved to: {output_filename}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c991dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score (Sastrawi) antara Docs1.txt and Docs2.txt: 0.6044\n",
      "Similarity Score (Sastrawi) antara Docs1.txt and Docs3.txt: 0.6064\n",
      "Similarity Score (Sastrawi) antara Docs1.txt and Docs4.txt: 0.6064\n",
      "Similarity Score (Sastrawi) antara Docs2.txt and Docs3.txt: 0.4574\n",
      "Similarity Score (Sastrawi) antara Docs2.txt and Docs4.txt: 0.4574\n",
      "Similarity Score (Sastrawi) antara Docs3.txt and Docs4.txt: 1.0000\n",
      "\n",
      "\n",
      "Similarity Score (Porter) antara Docs1.txt and Docs2.txt: 0.6105\n",
      "Similarity Score (Porter) antara Docs1.txt and Docs3.txt: 0.6344\n",
      "Similarity Score (Porter) antara Docs1.txt and Docs4.txt: 0.6344\n",
      "Similarity Score (Porter) antara Docs2.txt and Docs3.txt: 0.5543\n",
      "Similarity Score (Porter) antara Docs2.txt and Docs4.txt: 0.5543\n",
      "Similarity Score (Porter) antara Docs3.txt and Docs4.txt: 1.0000\n",
      "\n",
      "\n",
      "=== Perbandingan Stemming melalui Rabin Karp===\n",
      "\n",
      "PERBANDINGAN:\n",
      "Document     Sastrawi   Porter     Selisih   \n",
      "---------------------------------------------\n",
      "Docs1.txt    0.6057     0.6264     0.0207    \n",
      "Docs2.txt    0.5064     0.5731     0.0666    \n",
      "Docs3.txt    0.6879     0.7296     0.0416    \n",
      "Docs4.txt    0.6879     0.7296     0.0416    \n"
     ]
    }
   ],
   "source": [
    "def rabin_karp_ngrams(words, n, base=256, mod=101):\n",
    "    hashes = set()\n",
    "    if len(words) < n:\n",
    "        return hashes\n",
    "    h = 0\n",
    "    high_order = pow(base, n-1, mod)\n",
    "    \n",
    "    for i in range(n):\n",
    "        h = (h * base + ord(words[i][0])) % mod\n",
    "    hashes.add(h)\n",
    "\n",
    "    for i in range(1, len(words) - n + 1):\n",
    "        h = (h - ord(words[i-1][0]) * high_order) % mod\n",
    "        h = (h * base + ord(words[i+n-1][0])) % mod\n",
    "        hashes.add(h)\n",
    "    return hashes\n",
    "\n",
    "doc_scores_sastrawi = {doc: [] for doc in processed_documents_sastrawi}\n",
    "doc_names_sastrawi = list(processed_documents_sastrawi.keys())\n",
    "ngram_hashes_sastrawi = {doc: rabin_karp_ngrams(processed_documents_sastrawi[doc], n=3) for doc in doc_names_sastrawi}\n",
    "\n",
    "doc_scores_porter = {doc: [] for doc in processed_documents_porter}\n",
    "doc_names_porter = list(processed_documents_porter.keys())\n",
    "ngram_hashes_porter = {doc: rabin_karp_ngrams(processed_documents_porter[doc], n=3) for doc in doc_names_porter}\n",
    "\n",
    "for i in range(len(doc_names_sastrawi)):    \n",
    "    for j in range(i+1, len(doc_names_sastrawi)):\n",
    "        doc1, doc2 = doc_names_sastrawi[i], doc_names_sastrawi[j]\n",
    "        common = ngram_hashes_sastrawi[doc1] & ngram_hashes_sastrawi[doc2]\n",
    "        total = ngram_hashes_sastrawi[doc1] | ngram_hashes_sastrawi[doc2]\n",
    "        similarity = len(common) / len(total) if total else 0\n",
    "        doc_scores_sastrawi[doc1].append(similarity)\n",
    "        doc_scores_sastrawi[doc2].append(similarity)\n",
    "        print(f\"Similarity Score (Sastrawi) antara {doc1} and {doc2}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(len(doc_names_porter)):\n",
    "    for j in range(i+1, len(doc_names_porter)):\n",
    "        doc1, doc2 = doc_names_porter[i], doc_names_porter[j]\n",
    "        common = ngram_hashes_porter[doc1] & ngram_hashes_porter[doc2]\n",
    "        total = ngram_hashes_porter[doc1] | ngram_hashes_porter[doc2]\n",
    "        similarity = len(common) / len(total) if total else 0\n",
    "        doc_scores_porter[doc1].append(similarity)\n",
    "        doc_scores_porter[doc2].append(similarity)\n",
    "        print(f\"Similarity Score (Porter) antara {doc1} and {doc2}: {similarity:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=== Perbandingan Stemming melalui Rabin Karp===\")\n",
    "\n",
    "print(\"\\nPERBANDINGAN:\")\n",
    "print(f\"{'Document':<12} {'Sastrawi':<10} {'Porter':<10} {'Selisih':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for doc in doc_names_sastrawi:\n",
    "    sastrawi_score = sum(doc_scores_sastrawi[doc]) / len(doc_scores_sastrawi[doc]) if doc_scores_sastrawi[doc] else 0\n",
    "    porter_score = sum(doc_scores_porter[doc]) / len(doc_scores_porter[doc]) if doc_scores_porter[doc] else 0\n",
    "    difference = abs(sastrawi_score - porter_score)\n",
    "    print(f\"{doc:<12} {sastrawi_score:<10.4f} {porter_score:<10.4f} {difference:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc96f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membangun representasi TF-IDF (Sastrawi)...\n",
      "Membangun representasi TF-IDF (Porter)...\n",
      "\n",
      "Menghitung Cosine Similarity dengan TF-IDF (Sastrawi)...\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs1.txt dan Docs2.txt: 0.0893\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs1.txt dan Docs3.txt: 0.0453\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs1.txt dan Docs4.txt: 0.0453\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs2.txt dan Docs3.txt: 0.0590\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs2.txt dan Docs4.txt: 0.0590\n",
      "Cosine Similarity (TF-IDF Sastrawi) antara Docs3.txt dan Docs4.txt: 1.0000\n",
      "\n",
      "\n",
      "Menghitung Cosine Similarity dengan TF-IDF (Porter)...\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs1.txt dan Docs2.txt: 0.0698\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs1.txt dan Docs3.txt: 0.0221\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs1.txt dan Docs4.txt: 0.0221\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs2.txt dan Docs3.txt: 0.1398\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs2.txt dan Docs4.txt: 0.1398\n",
      "Cosine Similarity (TF-IDF Porter) antara Docs3.txt dan Docs4.txt: 1.0000\n",
      "\n",
      "\n",
      "=== Perbandingan Stemming melalui Cosine Similarity (TF-IDF) ===\n",
      "\n",
      "PERBANDINGAN RATA-RATA KESAMAAN PER DOKUMEN (TF-IDF):\n",
      "Document     Sastrawi   Porter     Selisih   \n",
      "---------------------------------------------\n",
      "Docs1.txt    0.0600     0.0380     0.0220    \n",
      "Docs2.txt    0.0691     0.1165     0.0474    \n",
      "Docs3.txt    0.3681     0.3873     0.0192    \n",
      "Docs4.txt    0.3681     0.3873     0.0192    \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# --- Fungsi Baru untuk TF-IDF dan Cosine Similarity ---\n",
    "\n",
    "def compute_tf(doc_words):\n",
    "    \"\"\"\n",
    "    Menghitung Term Frequency (TF) untuk satu dokumen.\n",
    "    \n",
    "    Args:\n",
    "        doc_words (list): Daftar kata (string) dalam satu dokumen.\n",
    "        \n",
    "    Returns:\n",
    "        collections.Counter: Kamus yang memetakan kata ke frekuensinya dalam dokumen.\n",
    "    \"\"\"\n",
    "    return Counter(doc_words)\n",
    "\n",
    "def compute_idf(all_docs_tf_list, num_total_docs, vocabulary):\n",
    "    \"\"\"\n",
    "    Menghitung Inverse Document Frequency (IDF) untuk setiap kata dalam vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        all_docs_tf_list (list): Daftar kamus TF untuk setiap dokumen. \n",
    "                                 Setiap elemen adalah Counter({word: freq}).\n",
    "        num_total_docs (int): Jumlah total dokumen dalam korpus.\n",
    "        vocabulary (set): Himpunan semua kata unik di seluruh korpus.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Kamus yang memetakan kata ke skor IDF-nya.\n",
    "    \"\"\"\n",
    "    df = Counter()  # Document Frequency for each term\n",
    "    for term in vocabulary:\n",
    "        for doc_tf in all_docs_tf_list:\n",
    "            if term in doc_tf and doc_tf[term] > 0:\n",
    "                df[term] += 1\n",
    "    \n",
    "    idf_scores = {}\n",
    "    for term in vocabulary:\n",
    "        \n",
    "        if df.get(term, 0) > 0:\n",
    "            idf_scores[term] = math.log(num_total_docs / df[term]) + 1 \n",
    "        else:\n",
    "            \n",
    "            idf_scores[term] = 1.0 # Atau bisa juga 0.0 atau math.log(num_total_docs / 1) + 1\n",
    "                                   # Jika vocab dibuat dari dokumen, df[term] akan selalu > 0\n",
    "    return idf_scores\n",
    "\n",
    "def generate_tfidf_representations(processed_documents):\n",
    "    \"\"\"\n",
    "    Menghasilkan representasi TF-IDF untuk semua dokumen.\n",
    "    \n",
    "    Args:\n",
    "        processed_documents (dict): Kamus {doc_name: [list of words]}.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Kamus {doc_name: {word: tfidf_score}}.\n",
    "    \"\"\"\n",
    "    doc_names = list(processed_documents.keys())\n",
    "    all_doc_word_lists = [processed_documents[name] for name in doc_names]\n",
    "    num_docs = len(all_doc_word_lists)\n",
    "\n",
    "    if num_docs == 0:\n",
    "        return {}\n",
    "\n",
    "    all_docs_tf_list = [compute_tf(doc_words) for doc_words in all_doc_word_lists]\n",
    "\n",
    "    vocabulary = set()\n",
    "    for doc_tf in all_docs_tf_list:\n",
    "        vocabulary.update(doc_tf.keys())\n",
    "    \n",
    "    if not vocabulary: # Jika tidak ada kata sama sekali di semua dokumen\n",
    "        return {name: {} for name in doc_names}\n",
    "\n",
    "    # 3. Hitung IDF\n",
    "    idf_scores = compute_idf(all_docs_tf_list, num_docs, vocabulary)\n",
    "    \n",
    "    # 4. Hitung vektor TF-IDF untuk setiap dokumen\n",
    "    tfidf_representations = {}\n",
    "    for i, doc_name in enumerate(doc_names):\n",
    "        doc_tf = all_docs_tf_list[i]  # Ini adalah Counter dari compute_tf\n",
    "        current_tfidf_vector = {}\n",
    "        for term, tf_value in doc_tf.items():\n",
    "            # Skor IDF mungkin tidak ada jika term di doc_tf tidak ada di vocab global (seharusnya tidak terjadi)\n",
    "            current_tfidf_vector[term] = tf_value * idf_scores.get(term, 0) \n",
    "        tfidf_representations[doc_name] = current_tfidf_vector\n",
    "        \n",
    "    return tfidf_representations\n",
    "\n",
    "def calculate_cosine_similarity(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Menghitung cosine similarity antara dua kamus (misalnya, {term: score}).\n",
    "    Fungsi ini sama seperti sebelumnya, hanya inputnya sekarang adalah kamus TF-IDF.\n",
    "    \n",
    "    Args:\n",
    "        dict1 (dict): Kamus representasi vektor untuk dokumen 1 ({item: score}).\n",
    "        dict2 (dict): Kamus representasi vektor untuk dokumen 2 ({item: score}).\n",
    "        \n",
    "    Returns:\n",
    "        float: Skor cosine similarity (antara 0.0 dan 1.0).\n",
    "    \"\"\"\n",
    "    all_items = set(dict1.keys()).union(set(dict2.keys()))\n",
    "    \n",
    "    if not all_items:\n",
    "        return 0.0  # Jika tidak ada item/kata sama sekali\n",
    "\n",
    "    vec1_values = []\n",
    "    vec2_values = []\n",
    "    \n",
    "    for item in all_items:\n",
    "        vec1_values.append(dict1.get(item, 0))\n",
    "        vec2_values.append(dict2.get(item, 0))\n",
    "        \n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1_values, vec2_values))\n",
    "    \n",
    "    magnitude1 = math.sqrt(sum(v1**2 for v1 in vec1_values))\n",
    "    magnitude2 = math.sqrt(sum(v2**2 for v2 in vec2_values))\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0  # Salah satu atau kedua vektor adalah nol\n",
    "        \n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "print(\"Membangun representasi TF-IDF (Sastrawi)...\")\n",
    "# Inisialisasi ulang kamus skor jika menjalankan beberapa kali dalam sesi yang sama\n",
    "doc_scores_sastrawi = {doc: [] for doc in processed_documents_sastrawi}\n",
    "doc_names_sastrawi = list(processed_documents_sastrawi.keys())\n",
    "tfidf_representations_sastrawi = generate_tfidf_representations(processed_documents_sastrawi)\n",
    "\n",
    "print(\"Membangun representasi TF-IDF (Porter)...\")\n",
    "doc_scores_porter = {doc: [] for doc in processed_documents_porter}\n",
    "doc_names_porter = list(processed_documents_porter.keys())\n",
    "tfidf_representations_porter = generate_tfidf_representations(processed_documents_porter)\n",
    "\n",
    "\n",
    "print(\"\\nMenghitung Cosine Similarity dengan TF-IDF (Sastrawi)...\")\n",
    "for i in range(len(doc_names_sastrawi)):     \n",
    "    for j in range(i + 1, len(doc_names_sastrawi)):\n",
    "        doc1_name, doc2_name = doc_names_sastrawi[i], doc_names_sastrawi[j]\n",
    "        \n",
    "        # Pastikan dokumen ada di representasi (seharusnya selalu ada jika diproses dengan benar)\n",
    "        vec1 = tfidf_representations_sastrawi.get(doc1_name, {})\n",
    "        vec2 = tfidf_representations_sastrawi.get(doc2_name, {})\n",
    "        \n",
    "        similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "        \n",
    "        doc_scores_sastrawi[doc1_name].append(similarity)\n",
    "        doc_scores_sastrawi[doc2_name].append(similarity)\n",
    "        print(f\"Cosine Similarity (TF-IDF Sastrawi) antara {doc1_name} dan {doc2_name}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Menghitung Cosine Similarity dengan TF-IDF (Porter)...\")\n",
    "for i in range(len(doc_names_porter)):\n",
    "    for j in range(i + 1, len(doc_names_porter)):\n",
    "        doc1_name, doc2_name = doc_names_porter[i], doc_names_porter[j]\n",
    "\n",
    "        vec1 = tfidf_representations_porter.get(doc1_name, {})\n",
    "        vec2 = tfidf_representations_porter.get(doc2_name, {})\n",
    "\n",
    "        similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "\n",
    "        doc_scores_porter[doc1_name].append(similarity)\n",
    "        doc_scores_porter[doc2_name].append(similarity)\n",
    "        print(f\"Cosine Similarity (TF-IDF Porter) antara {doc1_name} dan {doc2_name}: {similarity:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Perbandingan Hasil ---\n",
    "print(\"=== Perbandingan Stemming melalui Cosine Similarity (TF-IDF) ===\")\n",
    "\n",
    "print(\"\\nPERBANDINGAN RATA-RATA KESAMAAN PER DOKUMEN (TF-IDF):\")\n",
    "print(f\"{'Document':<12} {'Sastrawi':<10} {'Porter':<10} {'Selisih':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for doc_name_key in doc_names_sastrawi: \n",
    "    sastrawi_avg_score = 0\n",
    "    if doc_scores_sastrawi.get(doc_name_key) and len(doc_scores_sastrawi[doc_name_key]) > 0:\n",
    "        sastrawi_avg_score = sum(doc_scores_sastrawi[doc_name_key]) / len(doc_scores_sastrawi[doc_name_key])\n",
    "    \n",
    "    porter_avg_score = 0\n",
    "    if doc_name_key in doc_scores_porter:\n",
    "        if doc_scores_porter.get(doc_name_key) and len(doc_scores_porter[doc_name_key]) > 0:\n",
    "            porter_avg_score = sum(doc_scores_porter[doc_name_key]) / len(doc_scores_porter[doc_name_key])\n",
    "    \n",
    "    difference = abs(sastrawi_avg_score - porter_avg_score)\n",
    "    print(f\"{doc_name_key:<12} {sastrawi_avg_score:<10.4f} {porter_avg_score:<10.4f} {difference:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6d9169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Menghitung Jaccard Similarity (Rabin-Karp) ---\n",
      "Memproses Sastrawi (Jaccard)...\n",
      "Memproses Porter (Jaccard)...\n",
      "Memproses Porter (TF-IDF)...\n",
      "\n",
      "\n",
      "=== Perbandingan Gabungan Skor Rata-Rata Similarity Antar Dokumen ===\n",
      "Document        | Sastrawi (rabin)        | Porter (rabin)          | Sastrawi (cosine)       | Porter (cosine)        \n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Docs1.txt       | 0.6057                  | 0.6264                  | 0.0600                  | 0.0380                 \n",
      "Docs2.txt       | 0.5064                  | 0.5731                  | 0.0691                  | 0.1165                 \n",
      "Docs3.txt       | 0.6879                  | 0.7296                  | 0.3681                  | 0.3873                 \n",
      "Docs4.txt       | 0.6879                  | 0.7296                  | 0.3681                  | 0.3873                 \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# --- BAGIAN 1: Fungsi dan Perhitungan Jaccard via Rabin-Karp ---\n",
    "def rabin_karp_ngrams(words, n, base=256, mod=101):\n",
    "    \"\"\"\n",
    "    Menghasilkan satu set hash n-gram dari karakter pertama kata-kata dalam daftar kata.\n",
    "    Ini adalah fungsi dari kode asli Anda.\n",
    "    \"\"\"\n",
    "    hashes = set()\n",
    "    # Memastikan setiap 'kata' dalam 'words' adalah string dan tidak kosong\n",
    "    # sebelum mengambil karakter pertamanya.\n",
    "    # Juga, memastikan 'words' itu sendiri tidak kosong.\n",
    "    if not words:\n",
    "        return hashes\n",
    "        \n",
    "    valid_first_chars = [word[0] for word in words if isinstance(word, str) and len(word) > 0]\n",
    "\n",
    "    if len(valid_first_chars) < n:\n",
    "        return hashes\n",
    "    \n",
    "    h = 0\n",
    "    high_order = pow(base, n - 1, mod)  # base^(n-1) % mod\n",
    "    \n",
    "    # Hitung hash untuk n-gram pertama dari karakter pertama kata-kata\n",
    "    for i in range(n):\n",
    "        h = (h * base + ord(valid_first_chars[i])) % mod \n",
    "    hashes.add(h)\n",
    "\n",
    "    # Hitung hash untuk n-gram berikutnya menggunakan rolling hash\n",
    "    for i in range(1, len(valid_first_chars) - n + 1):\n",
    "        h = (h - ord(valid_first_chars[i-1]) * high_order) % mod\n",
    "        h = (h * base + ord(valid_first_chars[i+n-1])) % mod\n",
    "        h = (h + mod) % mod  # pastikan hash selalu positif\n",
    "        hashes.add(h)\n",
    "    return hashes\n",
    "\n",
    "N_GRAM_SIZE_RABIN_KARP = 3  # Sesuaikan jika perlu\n",
    "\n",
    "# -- Jaccard Sastrawi --\n",
    "print(\"--- Menghitung Jaccard Similarity (Rabin-Karp) ---\")\n",
    "doc_scores_jaccard_sastrawi = {doc: [] for doc in processed_documents_sastrawi}\n",
    "doc_names_sastrawi = list(processed_documents_sastrawi.keys())\n",
    "ngram_hashes_sastrawi = {doc: rabin_karp_ngrams(processed_documents_sastrawi[doc], n=N_GRAM_SIZE_RABIN_KARP) for doc in doc_names_sastrawi}\n",
    "\n",
    "print(\"Memproses Sastrawi (Jaccard)...\")\n",
    "for i in range(len(doc_names_sastrawi)):\n",
    "    for j in range(i + 1, len(doc_names_sastrawi)):\n",
    "        doc1, doc2 = doc_names_sastrawi[i], doc_names_sastrawi[j]\n",
    "        common = ngram_hashes_sastrawi[doc1] & ngram_hashes_sastrawi[doc2]\n",
    "        total = ngram_hashes_sastrawi[doc1] | ngram_hashes_sastrawi[doc2]\n",
    "        similarity = len(common) / len(total) if total else 0.0\n",
    "        doc_scores_jaccard_sastrawi[doc1].append(similarity)\n",
    "        doc_scores_jaccard_sastrawi[doc2].append(similarity)\n",
    "\n",
    "avg_jaccard_sastrawi = {}\n",
    "for doc_name in doc_names_sastrawi:\n",
    "    scores = doc_scores_jaccard_sastrawi[doc_name]\n",
    "    avg_jaccard_sastrawi[doc_name] = sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "# -- Jaccard Porter --\n",
    "doc_scores_jaccard_porter = {doc: [] for doc in processed_documents_porter}\n",
    "doc_names_porter = list(processed_documents_porter.keys())\n",
    "ngram_hashes_porter = {doc: rabin_karp_ngrams(processed_documents_porter[doc], n=N_GRAM_SIZE_RABIN_KARP) for doc in doc_names_porter}\n",
    "\n",
    "print(\"Memproses Porter (Jaccard)...\")\n",
    "for i in range(len(doc_names_porter)):\n",
    "    for j in range(i + 1, len(doc_names_porter)):\n",
    "        doc1, doc2 = doc_names_porter[i], doc_names_porter[j]\n",
    "        common = ngram_hashes_porter[doc1] & ngram_hashes_porter[doc2]\n",
    "        total = ngram_hashes_porter[doc1] | ngram_hashes_porter[doc2]\n",
    "        similarity = len(common) / len(total) if total else 0.0\n",
    "        doc_scores_jaccard_porter[doc1].append(similarity)\n",
    "        doc_scores_jaccard_porter[doc2].append(similarity)\n",
    "\n",
    "avg_jaccard_porter = {}\n",
    "for doc_name in doc_names_porter:\n",
    "    scores = doc_scores_jaccard_porter[doc_name]\n",
    "    avg_jaccard_porter[doc_name] = sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "\n",
    "def compute_tf(doc_words):\n",
    "    return Counter(doc_words)\n",
    "\n",
    "def compute_idf(all_docs_tf_list, num_total_docs, vocabulary):\n",
    "    df = Counter()\n",
    "    for term in vocabulary:\n",
    "        for doc_tf in all_docs_tf_list:\n",
    "            if term in doc_tf and doc_tf[term] > 0:\n",
    "                df[term] += 1\n",
    "    idf_scores = {}\n",
    "    for term in vocabulary:\n",
    "        if df.get(term, 0) > 0:\n",
    "            idf_scores[term] = math.log(num_total_docs / df[term]) + 1\n",
    "        else:\n",
    "            idf_scores[term] = 1.0 \n",
    "    return idf_scores\n",
    "\n",
    "def generate_tfidf_representations(processed_documents_map): # Ganti nama parameter agar tidak konflik\n",
    "    doc_names_list = list(processed_documents_map.keys())\n",
    "    all_doc_word_lists = [processed_documents_map[name] for name in doc_names_list]\n",
    "    num_docs = len(all_doc_word_lists)\n",
    "\n",
    "    if num_docs == 0: return {}\n",
    "    all_docs_tf_list = [compute_tf(doc_words) for doc_words in all_doc_word_lists]\n",
    "    vocabulary = set()\n",
    "    for doc_tf in all_docs_tf_list: vocabulary.update(doc_tf.keys())\n",
    "    if not vocabulary: return {name: {} for name in doc_names_list}\n",
    "    \n",
    "    idf_scores = compute_idf(all_docs_tf_list, num_docs, vocabulary)\n",
    "    tfidf_representations = {}\n",
    "    for i, doc_name in enumerate(doc_names_list):\n",
    "        doc_tf = all_docs_tf_list[i]\n",
    "        current_tfidf_vector = {}\n",
    "        for term, tf_value in doc_tf.items():\n",
    "            current_tfidf_vector[term] = tf_value * idf_scores.get(term, 0)\n",
    "        tfidf_representations[doc_name] = current_tfidf_vector\n",
    "    return tfidf_representations\n",
    "\n",
    "def calculate_cosine_similarity(dict1, dict2):\n",
    "    all_items = set(dict1.keys()).union(set(dict2.keys()))\n",
    "    if not all_items: return 0.0\n",
    "    vec1_values = [dict1.get(item, 0) for item in all_items]\n",
    "    vec2_values = [dict2.get(item, 0) for item in all_items]\n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1_values, vec2_values))\n",
    "    magnitude1 = math.sqrt(sum(v1**2 for v1 in vec1_values))\n",
    "    magnitude2 = math.sqrt(sum(v2**2 for v2 in vec2_values))\n",
    "    if magnitude1 == 0 or magnitude2 == 0: return 0.0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "doc_scores_tfidf_sastrawi = {doc: [] for doc in processed_documents_sastrawi} # Nama variabel berbeda\n",
    "tfidf_representations_sastrawi = generate_tfidf_representations(processed_documents_sastrawi)\n",
    "\n",
    "for i in range(len(doc_names_sastrawi)): # Bisa gunakan doc_names_sastrawi yang sudah ada\n",
    "    for j in range(i + 1, len(doc_names_sastrawi)):\n",
    "        doc1_name, doc2_name = doc_names_sastrawi[i], doc_names_sastrawi[j]\n",
    "        vec1 = tfidf_representations_sastrawi.get(doc1_name, {})\n",
    "        vec2 = tfidf_representations_sastrawi.get(doc2_name, {})\n",
    "        similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "        doc_scores_tfidf_sastrawi[doc1_name].append(similarity)\n",
    "        doc_scores_tfidf_sastrawi[doc2_name].append(similarity)\n",
    "\n",
    "avg_tfidf_sastrawi = {}\n",
    "for doc_name in doc_names_sastrawi:\n",
    "    scores = doc_scores_tfidf_sastrawi[doc_name]\n",
    "    avg_tfidf_sastrawi[doc_name] = sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "doc_scores_tfidf_porter = {doc: [] for doc in processed_documents_porter} # Nama variabel berbeda\n",
    "tfidf_representations_porter = generate_tfidf_representations(processed_documents_porter)\n",
    "\n",
    "print(\"Memproses Porter (TF-IDF)...\")\n",
    "for i in range(len(doc_names_porter)): # Bisa gunakan doc_names_porter yang sudah ada\n",
    "    for j in range(i + 1, len(doc_names_porter)):\n",
    "        doc1_name, doc2_name = doc_names_porter[i], doc_names_porter[j]\n",
    "        vec1 = tfidf_representations_porter.get(doc1_name, {})\n",
    "        vec2 = tfidf_representations_porter.get(doc2_name, {})\n",
    "        similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "        doc_scores_tfidf_porter[doc1_name].append(similarity)\n",
    "        doc_scores_tfidf_porter[doc2_name].append(similarity)\n",
    "\n",
    "avg_tfidf_porter = {}\n",
    "for doc_name in doc_names_porter:\n",
    "    scores = doc_scores_tfidf_porter[doc_name]\n",
    "    avg_tfidf_porter[doc_name] = sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== Perbandingan Gabungan Skor Rata-Rata Similarity Antar Dokumen ===\")\n",
    "\n",
    "col_width_doc = 15\n",
    "col_width_score = 23 # Cukup untuk \"Sastrawi (Jaccard/TF-IDF)\"\n",
    "header_doc = f\"{'Document':<{col_width_doc}}\"\n",
    "header_s_jaccard = f\"{'Sastrawi (rabin)':<{col_width_score}}\"\n",
    "header_p_jaccard = f\"{'Porter (rabin)':<{col_width_score}}\"\n",
    "header_s_tfidf = f\"{'Sastrawi (cosine)':<{col_width_score}}\"\n",
    "header_p_tfidf = f\"{'Porter (cosine)':<{col_width_score}}\"\n",
    "\n",
    "header = f\"{header_doc} | {header_s_jaccard} | {header_p_jaccard} | {header_s_tfidf} | {header_p_tfidf}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Mengambil daftar nama dokumen unik dari kedua set untuk iterasi\n",
    "# Ini lebih aman jika nama dokumen bisa berbeda antara Sastrawi dan Porter\n",
    "all_doc_names_for_table = sorted(list(set(doc_names_sastrawi) | set(doc_names_porter)))\n",
    "\n",
    "\n",
    "for doc_name in all_doc_names_for_table:\n",
    "    score_s_jaccard = avg_jaccard_sastrawi.get(doc_name, 0.0)\n",
    "    score_p_jaccard = avg_jaccard_porter.get(doc_name, 0.0)\n",
    "    score_s_tfidf = avg_tfidf_sastrawi.get(doc_name, 0.0)\n",
    "    score_p_tfidf = avg_tfidf_porter.get(doc_name, 0.0)\n",
    "\n",
    "    row = (f\"{doc_name:<{col_width_doc}} | \"\n",
    "           f\"{score_s_jaccard:<{col_width_score}.4f} | \"\n",
    "           f\"{score_p_jaccard:<{col_width_score}.4f} | \"\n",
    "           f\"{score_s_tfidf:<{col_width_score}.4f} | \"\n",
    "           f\"{score_p_tfidf:<{col_width_score}.4f}\")\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
